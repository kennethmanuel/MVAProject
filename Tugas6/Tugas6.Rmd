---
title: "Salespeople-data"
author: "Kenneth Manuel"
date: "3/27/2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{R library, include=FALSE}
library("tidyverse")
library("readxl")
library("NbClust")
library("factoextra")
library("MASS")
library("caret")
library("MASS")
```

```{r}
data <- read_excel('Salespeople-data.xlsx')
head(data)
```

# Matriks korelasi

```{r}
res <- cor(data)
round(res, 2)
```

# Eigenvalue

```{r}
ev <- eigen(res)
e.values <- ev$values
round(e.values, 2)
```

# Menentukan keputusan berapa kluster yang akan digunakan

```{r}
fviz_nbclust(data, kmeans, method = "wss") +
    geom_vline(xintercept = 4 , linetype = 2)+
  labs(subtitle = "Elbow method")

fviz_nbclust(data, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

fviz_nbclust(data, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
  labs(subtitle = "Gap statistic method")
```

Baik metode elbow maupun sillhouete menyarankan untuk menggunakan 2 cluster sedangkan metode gap statistics menyarankan untuk menggunakan 4 cluster.

Karena dua metode yaitu sillhouete dan elbow menyarankan untuk menggunakan 2 cluster maka clustering akan dibagi menjadi **2 cluster**

# K-means clustering

```{r}
km.res <- eclust(data, "kmeans", k = 4,
                 nstart = 25, graph = FALSE)
# k-means group number of each observation
km.res$cluster
```

```{r}
fviz_cluster(km.res,  ellipse.type = "norm")
#fviz_silhouette(km.res)
```

# Hierachical clustering

```{r}
res.hc <- eclust(data, "hclust", k = 4,
                method = "single", graph = FALSE) 
res.hc$cluster
```

```{r}
fviz_dend(res.hc, rect = TRUE, show_labels = TRUE, cex = 0.5) 
#fviz_silhouette(res.hc)
```

# Comparing hierachical clustering and K-means clustering

```{r}
km.res$cluster
res.hc$cluster
```

Dari dapat dilihat bahwa sebenarnya kedua cluster memberikan kelas yang sama persis hanya saja hierachical clustering melabel kelas pertama k-means dengan kelas 1 dan kelas ke dua dengan kelas 2.

Attaching cluster result label to data

```{r}
cluster.member1 <- data.frame(cluster = km.res$cluster)
data.c1 <- cbind(data, cluster.member1)

cluster.member2 <- data.frame(cluster = res.hc$cluster)
data.c2 <- cbind(data, cluster.member2)
```

# Linear discriminant analysis hasil K-Means

Dengan mengasumsi bahwa masing - masing variabel berdistribusi normal univariate.

## 1. Data preparation

### Split data to training and test set

```{r}
# Split the data into training (80%) and test set (20%)
set.seed(123)
training.samples <- data.c1$cluster %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data <- data.c1[training.samples, ]
test.data <- data.c1[-training.samples, ]
```

### Normalize data

```{r}
# Estimate preprocessing parameters
preproc.param <- train.data %>% 
  preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
train.transformed <- preproc.param %>% predict(train.data)
test.transformed <- preproc.param %>% predict(test.data)

```

### Compute LDA

```{r}
model <- lda(cluster~., data = train.transformed)
model
```

## 2. Make predictions

```{r}
predictions <- model %>% predict(test.data)
names(predictions)
```

## 3. Model accuracy

```{r}
mean(predictions$class==test.transformed$cluster)
```

# Linear discriminant analysis hasil Hierachical clustering

Dengan mengasumsi bahwa masing - masing variabel berdistribusi normal univariate.

## 1. Data preparation

### Split data to training and test set

```{r}
# Split the data into training (80%) and test set (20%)
set.seed(123)
training.samples <- data.c2$cluster %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data <- data.c1[training.samples, ]
test.data <- data.c1[-training.samples, ]
```

### Normalize data

```{r}
# Estimate preprocessing parameters
preproc.param <- train.data %>% 
  preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
train.transformed <- preproc.param %>% predict(train.data)
test.transformed <- preproc.param %>% predict(test.data)

```

### Compute LDA

```{r}
model <- lda(cluster~., data = train.transformed)
model
```

## 2. Make predictions

```{r}
predictions <- model %>% predict(test.data)
names(predictions)
```

## 3. Model accuracy

```{r}
mean(predictions$class==test.transformed$cluster)
```
